<!DOCTYPE html> <html lang="en, de"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Segmentation and Semantic Classification with Pretrained Models | Kavin Palanichamy </title> <meta name="author" content="Kavin Palanichamy"> <meta name="description" content="An advanced computer vision system combining Meta's SAM (Segment Anything Model) and OpenAI's CLIP for intelligent object detection, segmentation, and natural language-based identification in images."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://kavinpalanichamy.github.io/projects/8_projects/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Kavin</span> Palanichamy </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Segmentation and Semantic Classification with Pretrained Models</h1> <p class="post-description">An advanced computer vision system combining Meta's SAM (Segment Anything Model) and OpenAI's CLIP for intelligent object detection, segmentation, and natural language-based identification in images.</p> </header> <article> <p>This project presents an intelligent animal detection and classification system that combines state-of-the-art computer vision models to identify and locate animals in images using natural language prompts. The system leverages Meta’s Segment Anything Model (SAM) for precise object segmentation and OpenAI’s CLIP for semantic understanding, creating a powerful tool for wildlife monitoring and research applications.</p> <div class="row justify-content-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <video src="/assets/video/8_1.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""></video> </figure> </div> </div> <div class="caption text-center"> Video 1. Real-time animal detection and classification demonstration. </div> <h2 id="system-overview">System Overview</h2> <p>The AI-powered animal detection system addresses the challenge of identifying and locating specific animals in complex natural scenes using intuitive natural language descriptions. Instead of requiring users to know exact taxonomic names, the system accepts descriptive prompts like “King of the jungle” or “Fastest land animal” and intelligently matches them to detected animals.</p> <p><strong>Key Capabilities:</strong></p> <ul> <li> <strong>Intelligent Segmentation:</strong> Uses SAM to precisely segment individual objects in images</li> <li> <strong>Natural Language Understanding:</strong> Accepts descriptive prompts for animal identification</li> <li> <strong>High-Precision Classification:</strong> Employs CLIP for robust animal classification across 12 species</li> <li> <strong>Spatial Localization:</strong> Provides detailed coordinate information for detected animals</li> <li> <strong>Visual Feedback:</strong> Generates annotated output images with bounding boxes and labels</li> </ul> <h2 id="technical-architecture">Technical Architecture</h2> <h3 id="core-components">Core Components</h3> <p><strong>1. Segment Anything Model (SAM)</strong></p> <ul> <li> <strong>Model:</strong> ViT-H (Huge variant) for maximum accuracy</li> <li> <strong>Function:</strong> Generates high-quality object masks without class-specific training</li> <li> <strong>Optimization:</strong> Configured with adaptive parameters to reduce over-segmentation</li> <li> <strong>Performance:</strong> Processes images with 32 seed points per side for comprehensive coverage</li> </ul> <p><strong>2. CLIP (Contrastive Language-Image Pre-training)</strong></p> <ul> <li> <strong>Model:</strong> ViT-L-14 for superior visual-semantic understanding</li> <li> <strong>Function:</strong> Classifies segmented objects and matches natural language prompts</li> <li> <strong>Classes:</strong> Supports 12 animal categories (zebra, giraffe, camel, moose, tiger, bear, cheetah, lion, gorilla, rhino, hippo, elephant)</li> <li> <strong>Confidence Filtering:</strong> Implements 30% confidence threshold for reliable detections</li> </ul> <p><strong>3. Advanced Processing Pipeline</strong></p> <ul> <li> <strong>Image Preprocessing:</strong> OpenCV-based image loading and color space conversion</li> <li> <strong>Mask Filtering:</strong> Multi-criteria filtering based on area, stability, and IoU scores</li> <li> <strong>Coordinate Extraction:</strong> Precise centroid and bounding box calculations</li> <li> <strong>Visual Overlay:</strong> PIL-based annotation with color-coded labels and coordinates</li> </ul> <h3 id="processing-workflow">Processing Workflow</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simplified workflow representation
</span><span class="mf">1.</span> <span class="n">Load</span> <span class="nb">input</span> <span class="n">image</span> <span class="err">→</span> <span class="n">Image</span> <span class="n">preprocessing</span>
<span class="mf">2.</span> <span class="n">Generate</span> <span class="nb">object</span> <span class="n">masks</span> <span class="err">→</span> <span class="n">SAM</span> <span class="n">segmentation</span>
<span class="mf">3.</span> <span class="n">Filter</span> <span class="n">high</span><span class="o">-</span><span class="n">quality</span> <span class="n">masks</span> <span class="err">→</span> <span class="n">Quality</span> <span class="n">assessment</span>
<span class="mf">4.</span> <span class="n">Extract</span> <span class="nb">object</span> <span class="n">crops</span> <span class="err">→</span> <span class="n">Bounding</span> <span class="n">box</span> <span class="n">extraction</span>
<span class="mf">5.</span> <span class="n">Classify</span> <span class="n">each</span> <span class="n">crop</span> <span class="err">→</span> <span class="n">CLIP</span> <span class="n">classification</span>
<span class="mf">6.</span> <span class="n">Match</span> <span class="n">user</span> <span class="n">prompt</span> <span class="err">→</span> <span class="n">Semantic</span> <span class="n">matching</span>
<span class="mf">7.</span> <span class="n">Generate</span> <span class="n">output</span> <span class="n">visualization</span> <span class="err">→</span> <span class="n">Result</span> <span class="n">overlay</span>
</code></pre></div></div> <h2 id="implementation-features">Implementation Features</h2> <h3 id="smart-segmentation-parameters">Smart Segmentation Parameters</h3> <p>The system employs optimized SAM parameters to balance detection accuracy with processing efficiency:</p> <ul> <li> <strong>Points per side:</strong> 32 seed points for comprehensive coverage</li> <li> <strong>IoU threshold:</strong> 0.90 for high-quality mask selection</li> <li> <strong>Stability score:</strong> 0.97 for consistent object boundaries</li> <li> <strong>Minimum area:</strong> 10,000 pixels to filter noise</li> <li> <strong>Maximum segments:</strong> Limited to 15 top-quality masks</li> </ul> <h3 id="confidence-based-filtering">Confidence-Based Filtering</h3> <p>Only detections exceeding the confidence threshold are considered valid, ensuring reliable results:</p> <ul> <li> <strong>Primary threshold:</strong> 30% confidence minimum</li> <li> <strong>Area filtering:</strong> Minimum 3,000 pixels for object significance</li> <li> <strong>Stability requirements:</strong> High stability scores for consistent detection</li> <li> <strong>Non-maximum suppression:</strong> Eliminates overlapping detections</li> </ul> <h3 id="coordinate-system">Coordinate System</h3> <p>The system provides comprehensive spatial information:</p> <ul> <li> <strong>Absolute coordinates:</strong> Standard image coordinate system</li> <li> <strong>Center-origin coordinates:</strong> Image center as (0,0) for robotics applications</li> <li> <strong>Bounding box dimensions:</strong> Width, height, and area measurements</li> <li> <strong>Centroid calculation:</strong> Precise center-of-mass positioning</li> </ul> <h2 id="user-interface-and-experience">User Interface and Experience</h2> <h3 id="interactive-command-line-interface">Interactive Command-Line Interface</h3> <p>The system features a sophisticated CLI with color-coded output for enhanced user experience:</p> <ul> <li> <strong>Progress tracking:</strong> Real-time progress bars for long operations</li> <li> <strong>Formatted output:</strong> Hierarchical information display with color coding</li> <li> <strong>Error handling:</strong> Comprehensive error messages with troubleshooting guidance</li> <li> <strong>Result summary:</strong> Detailed processing statistics and coordinate information</li> </ul> <h3 id="natural-language-processing">Natural Language Processing</h3> <p>Users can input descriptive prompts that are intelligently matched to animal classes:</p> <p><strong>Example Prompts:</strong></p> <ul> <li>“King of the jungle” → Lion</li> <li>“Fastest land animal” → Cheetah</li> <li>“Gentle giant” → Elephant</li> <li>“Striped horse” → Zebra</li> </ul> <h2 id="performance-and-results">Performance and Results</h2> <h3 id="detection-accuracy">Detection Accuracy</h3> <p>The system demonstrates robust performance across various scenarios:</p> <ul> <li> <strong>Segmentation quality:</strong> High-precision object boundaries using SAM</li> <li> <strong>Classification accuracy:</strong> Reliable species identification using CLIP</li> <li> <strong>Prompt matching:</strong> Intelligent semantic understanding of natural language</li> <li> <strong>Spatial precision:</strong> Accurate coordinate extraction for robotics applications</li> </ul> <h3 id="processing-metrics">Processing Metrics</h3> <ul> <li> <strong>Image processing:</strong> Supports various image formats and resolutions</li> <li> <strong>Real-time capability:</strong> Efficient processing for practical applications</li> <li> <strong>Memory optimization:</strong> CUDA acceleration when available</li> <li> <strong>Scalability:</strong> Configurable parameters for different use cases</li> </ul> <h2 id="applications-and-use-cases">Applications and Use Cases</h2> <h3 id="wildlife-monitoring">Wildlife Monitoring</h3> <ul> <li> <strong>Conservation research:</strong> Automated species counting and tracking</li> <li> <strong>Habitat studies:</strong> Animal distribution and behavior analysis</li> <li> <strong>Camera trap analysis:</strong> Automated processing of wildlife photographs</li> </ul> <h3 id="educational-tools">Educational Tools</h3> <ul> <li> <strong>Interactive learning:</strong> Natural language-based animal identification</li> <li> <strong>Zoo applications:</strong> Visitor-friendly animal information systems</li> <li> <strong>Research training:</strong> Computer vision education and demonstration</li> </ul> <h3 id="robotics-integration">Robotics Integration</h3> <ul> <li> <strong>Autonomous systems:</strong> Animal detection for navigation and monitoring</li> <li> <strong>Drone applications:</strong> Aerial wildlife surveys and tracking</li> <li> <strong>Coordinate systems:</strong> Ready-to-use spatial information for robotic control</li> </ul> <h2 id="technical-requirements">Technical Requirements</h2> <h3 id="hardware-specifications">Hardware Specifications</h3> <ul> <li> <strong>GPU:</strong> CUDA-compatible graphics card (recommended)</li> <li> <strong>RAM:</strong> Minimum 8GB, 16GB recommended for large images</li> <li> <strong>Storage:</strong> ~3GB for model weights and dependencies</li> <li> <strong>CPU:</strong> Multi-core processor for efficient processing</li> </ul> <h3 id="software-dependencies">Software Dependencies</h3> <ul> <li> <strong>Python 3.8+</strong> with computer vision libraries</li> <li> <strong>PyTorch</strong> for deep learning model execution</li> <li> <strong>OpenCV</strong> for image processing operations</li> <li> <strong>PIL/Pillow</strong> for image manipulation and annotation</li> <li> <strong>OpenCLIP</strong> for CLIP model implementation</li> <li> <strong>Segment Anything</strong> for SAM model functionality</li> </ul> <h2 id="future-enhancements">Future Enhancements</h2> <h3 id="expanded-capabilities">Expanded Capabilities</h3> <ul> <li> <strong>Extended animal database:</strong> Support for additional species and marine life</li> <li> <strong>Real-time video processing:</strong> Live camera feed analysis</li> <li> <strong>Behavior recognition:</strong> Advanced AI for animal activity classification</li> <li> <strong>Multi-language support:</strong> International prompt processing</li> </ul> <h3 id="performance-optimizations">Performance Optimizations</h3> <ul> <li> <strong>Model quantization:</strong> Reduced memory footprint for edge deployment</li> <li> <strong>Batch processing:</strong> Multiple image analysis capabilities</li> <li> <strong>API integration:</strong> Web service deployment for remote access</li> <li> <strong>Mobile optimization:</strong> Smartphone and tablet compatibility</li> </ul> <h2 id="conclusion">Conclusion</h2> <p>This AI-powered animal detection system represents a significant advancement in applying state-of-the-art computer vision to practical wildlife monitoring and research applications. By combining the precision of SAM segmentation with the semantic understanding of CLIP, the system provides an intuitive and powerful tool for animal identification and localization.</p> <p>The natural language interface makes the technology accessible to researchers and educators without deep technical expertise, while the precise coordinate output enables integration with robotic systems and automated monitoring platforms. This project demonstrates the potential of combining multiple AI models to create more intelligent and user-friendly computer vision applications.</p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Kavin Palanichamy. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>